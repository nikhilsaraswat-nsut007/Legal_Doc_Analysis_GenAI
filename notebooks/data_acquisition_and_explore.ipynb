{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd98a41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helo world\n"
     ]
    }
   ],
   "source": [
    "print('helo world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3573199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acba7fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 PDF files: ['doc-1.pdf', 'doc-2.pdf', 'doc-3.pdf', 'doc-4.pdf', 'doc-5.pdf', 'doc-6.pdf', 'doc-7.pdf']\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Extracting text from: doc-1.pdf (3 pages) ---\n",
      "Extracted 5473 characters from doc-1.pdf\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "--- Extracting text from: doc-2.pdf (3 pages) ---\n",
      "Extracted 3115 characters from doc-2.pdf\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "--- Extracting text from: doc-3.pdf (3 pages) ---\n",
      "Extracted 4384 characters from doc-3.pdf\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "--- Extracting text from: doc-4.pdf (2 pages) ---\n",
      "Extracted 2531 characters from doc-4.pdf\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "--- Extracting text from: doc-5.pdf (2 pages) ---\n",
      "Extracted 2140 characters from doc-5.pdf\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "--- Extracting text from: doc-6.pdf (4 pages) ---\n",
      "Extracted 7678 characters from doc-6.pdf\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "--- Extracting text from: doc-7.pdf (7 pages) ---\n",
      "Extracted 26651 characters from doc-7.pdf\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Starting Basic Cleaning...\n",
      "==================================================\n",
      "\n",
      "--- Basic Cleaned Text Preview for doc-1.pdf (First 750 chars) ---\n",
      "SERVICE AGREEMENT This Service Agreement (Agreement) is made and entered into on [Date] by and between: Service Provider: [Service Provider Name], a [business entity, e.g., corporation, LLC] with its principal place of business at [Service Provider Address] (Provider), and Client: [Client Name], a [business entity or individual] with its principal place of business or residence at [Client Address] (Client). WHEREAS, the Provider offers professional services as described herein, and the Client desires to engage the Provider to perform such services under the terms and conditions set forth in this Agreement; NOW, THEREFORE, in consideration of the mutual promises and covenants con- tained herein, the parties agree as follows: 1. Services 1.De\n",
      "Total characters after basic cleaning: 5454\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "--- Basic Cleaned Text Preview for doc-2.pdf (First 750 chars) ---\n",
      "NON-DISCLOSURE AGREEMENT This Non-Disclosure Agreement (Agreement) is entered into on July 6, 2025, by and between: Company: Horizon Solutions Pvt. Ltd., a private limited company located at 789 Tech Park, Hyderabad, Telangana, India (Company), and Employee: Neha Patel, residing at 321 Sunrise Residency, Gachibowli, Hyder- abad, Telangana, India (Employee). WHEREAS, the Employee will be engaged as Marketing Manager and will have access to sensitive and proprietary information; WHEREAS, the Company seeks to safeguard such information; NOW, THEREFORE, the parties agree as follows: 1. Confidential Information 1.Definition. Confidential Information encompasses all non-public informa- tion, including but not limited to marketing strategies, clie\n",
      "Total characters after basic cleaning: 3101\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "--- Basic Cleaned Text Preview for doc-3.pdf (First 750 chars) ---\n",
      "NON-DISCLOSURE AGREEMENT This Non-Disclosure Agreement (Agreement) is made and entered into on July 5, 2025, by and between: Company: Apex Technologies Inc., a corporation with its principal place of busi- ness at 123 Innovation Drive, Bengaluru, Karnataka, India (Company), and Employee: Arjun Sharma, an individual residing at 456 Lotus Apartments, Ko- ramangala, Bengaluru, Karnataka, India (Employee). WHEREAS, the Company intends to employ the Employee in the role of Software Engineer, and in the course of employment, the Employee will have access to confidential and proprietary information; WHEREAS, the Company desires to protect such information from unauthorized disclosure; NOW, THEREFORE, in consideration of the Employees employment an\n",
      "Total characters after basic cleaning: 4367\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "--- Basic Cleaned Text Preview for doc-4.pdf (First 750 chars) ---\n",
      "NON-DISCLOSURE AGREEMENT This Non-Disclosure Agreement (Agreement) is made on July 7, 2025, by: Company: Innovate Systems Ltd., a corporation at 101 Cyber City, Gurugram, Haryana, India (Company), and Employee: Rohan Mehra, residing at 654 Orchid Towers, Sector 56, Gurugram, Haryana, India (Employee). WHEREAS, the Employees role as Data Analyst involves access to sensitive in- formation; WHEREAS, the Company requires protection of such information; NOW, THEREFORE, the parties agree: 1. Confidential Information 1.Definition. Confidential Information includes proprietary data such as prod- uct designs, financial records, and client contracts disclosed to the Employee. 2. Employee Responsibilities 1.Duties. The Employee shall perform Data Anal\n",
      "Total characters after basic cleaning: 2518\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "--- Basic Cleaned Text Preview for doc-5.pdf (First 750 chars) ---\n",
      "NON-DISCLOSURE AGREEMENT This Non-Disclosure Agreement (Agreement) is made on July 9, 2025, by: Company: Quantum Dynamics Ltd., a corporation at 555 Tech Hub, Pune, Ma- harashtra, India (Company), and Employee: Vikram Desai, at 123 Riverfront Villas, Baner, Pune, Maharashtra, India (Employee). WHEREAS, the Employees role as Systems Administrator grants access to confi- dential data; WHEREAS, the Company requires confidentiality protection; NOW, THEREFORE, the parties agree: 1. Confidential Information 1.Definition. Includes trade secrets, financials, and client information disclosed to the Employee. 2. Employee Obligations 1.Duties. Perform Systems Administrator tasks per Company instructions. 2.Non-Disclosure. No disclosure of Confidential\n",
      "Total characters after basic cleaning: 2132\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "--- Basic Cleaned Text Preview for doc-6.pdf (First 750 chars) ---\n",
      "Page 1 of 4 AGREEMENT OF PURCHASE This agreement is by and between __________________ (Institution ), and _____________________ ( Seller ). WHEREAS, the Seller desires to sell to the Institution a comprehensive collection of ___________________ (Collection ), whi ch is more particularly described in the attached inventory, Attachment A , which is incorporated herein by reference ; and, WHEREAS, the Institution deems it in its interest to acquire the Collection for custodial care and appropriate service to the public , and is agreeable to purchasing the Collection under the terms hereafter stated; NOW, THEREFORE, the parties hereby agree as follows: 1) Purchase . The Seller agrees to sell, and the Institution agrees to buy, the Collection fo\n",
      "Total characters after basic cleaning: 7340\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "--- Basic Cleaned Text Preview for doc-7.pdf (First 750 chars) ---\n",
      "Ninu, Inc. Public Domain Agreement v.1.01 1 PUBLIC DOMAIN AGREEMENT THIS IS A LEGAL AGREEMENT (\"Agreement\") BETWEEN YOU AND NINU, INC. (NIMIA). PLEASE READ THIS AGREEMENT IN ITS ENTIRETY BEFORE YOU DOWNLOAD OR ACCESS ANY CONTENT CATEGORIZED AS PUBLIC DOMAIN (CONTENT) ON NIMIA OWNED WEB PROPERTIES. BY DOWNLOADING OR RECEIVING ANY CONTENT CATEGORIZED AS PUBLIC DOMAIN FROM NIMIA OWNED WEB PROPERTIES YOU AGREE TO BE BOUND BY THE TERMS OF THIS AGREEMENT. The Public Domain Agreement (Agreement) is a contract between the You and Ninu, Inc. (Nimia), 701 5th Avenue, Suite 4200, Seattle WA, 98104. The Agreement is in addition to the Producer Agreement, Site User Agreement, Cloud Storage Agreement, Application License Agreement, and Community Section \n",
      "Total characters after basic cleaning: 16986\n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import PyPDF2\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Define the path to your raw documents folder\n",
    "raw_docs_folder = os.path.join('..', 'data', 'raw_documents')\n",
    "\n",
    "# Get a list of all PDF files in the folder\n",
    "pdf_files_in_folder = sorted([f for f in os.listdir(raw_docs_folder) if f.lower().endswith('.pdf')])\n",
    "\n",
    "# Dictionary to store extracted text before cleaning\n",
    "raw_extracted_texts = {}\n",
    "\n",
    "if not pdf_files_in_folder:\n",
    "    print(f\"No PDF files found in {raw_docs_folder}. Please ensure your files are in the correct directory.\")\n",
    "else:\n",
    "    print(f\"Found {len(pdf_files_in_folder)} PDF files: {pdf_files_in_folder}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\") # Separator for clarity\n",
    "\n",
    "    for pdf_file_name in pdf_files_in_folder:\n",
    "        pdf_path = os.path.join(raw_docs_folder, pdf_file_name)\n",
    "        extracted_text = \"\"\n",
    "\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                num_pages = len(reader.pages)\n",
    "                print(f\"--- Extracting text from: {pdf_file_name} ({num_pages} pages) ---\")\n",
    "\n",
    "                for page_num in range(num_pages):\n",
    "                    page = reader.pages[page_num]\n",
    "                    extracted_text += page.extract_text() + \"\\n\" # Add newline to separate page content\n",
    "            \n",
    "            raw_extracted_texts[pdf_file_name] = extracted_text\n",
    "            print(f\"Extracted {len(extracted_text)} characters from {pdf_file_name}\")\n",
    "            print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File not found at {pdf_path}. Check name and path.\")\n",
    "        except PyPDF2.errors.PdfReadError:\n",
    "            print(f\"Error: Could not read {pdf_file_name}. It might be corrupted or encrypted.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred with {pdf_file_name}: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"Starting Basic Cleaning...\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Dictionary to store cleaned text\n",
    "cleaned_texts = {}\n",
    "\n",
    "# This loop now correctly uses raw_extracted_texts which is populated above\n",
    "for file_name, text_content in raw_extracted_texts.items():\n",
    "    # --- Basic Cleaning Steps ---\n",
    "\n",
    "    # Step 1: Replace multiple whitespace characters (spaces, tabs, newlines) with a single space.\n",
    "    # This handles excessive newlines and tabs very effectively.\n",
    "    text = re.sub(r'\\s+', ' ', text_content)\n",
    "\n",
    "    # Step 2: Remove non-ASCII characters. These often appear as junk from PDF extraction.\n",
    "    # This keeps only standard English letters, numbers, and basic punctuation.\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "    # Step 3: Remove leading and trailing whitespace from the entire document.\n",
    "    text = text.strip()\n",
    "\n",
    "    # Store the partially cleaned text\n",
    "    cleaned_texts[file_name] = text\n",
    "\n",
    "    # Print a preview of the cleaned text\n",
    "    print(f\"--- Basic Cleaned Text Preview for {file_name} (First 750 chars) ---\")\n",
    "    print(cleaned_texts[file_name][:750])\n",
    "    print(f\"Total characters after basic cleaning: {len(cleaned_texts[file_name])}\")\n",
    "    print(\"\\n\" + \"-\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c888676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "Starting Advanced Cleaning...\n",
      "==================================================\n",
      "\n",
      "--- FINAL Cleaned Text Preview for doc-1.pdf (First 750 chars) ---\n",
      "SERVICE AGREEMENT This Service Agreement (Agreement) is made and entered into on [Date] by and between: Service Provider: [Service Provider Name], a [business entity, e.g., corporation, LLC] with its principal place of business at [Service Provider Address] (Provider), and Client: [Client Name], a [business entity or individual] with its principal place of business or residence at [Client Address] (Client). WHEREAS, the Provider offers professional services as described herein, and the Client desires to engage the Provider to perform such services under the terms and conditions set forth in this Agreement; NOW, THEREFORE, in consideration of the mutual promises and covenants contained herein, the parties agree as follows: 1. Services 1.Desc\n",
      "Total characters after final cleaning: 5419\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- FINAL Cleaned Text Preview for doc-2.pdf (First 750 chars) ---\n",
      "NONDISCLOSURE AGREEMENT This NonDisclosure Agreement (Agreement) is entered into on July 6, 2025, by and between: Company: Horizon Solutions Pvt. Ltd., a private limited company located at 789 Tech Park, Hyderabad, Telangana, India (Company), and Employee: Neha Patel, residing at 321 Sunrise Residency, Gachibowli, Hyderabad, Telangana, India (Employee). WHEREAS, the Employee will be engaged as Marketing Manager and will have access to sensitive and proprietary information; WHEREAS, the Company seeks to safeguard such information; NOW, THEREFORE, the parties agree as follows: 1. Confidential Information 1.Definition. Confidential Information encompasses all nonpublic information, including but not limited to marketing strategies, client data\n",
      "Total characters after final cleaning: 3074\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- FINAL Cleaned Text Preview for doc-3.pdf (First 750 chars) ---\n",
      "NONDISCLOSURE AGREEMENT This NonDisclosure Agreement (Agreement) is made and entered into on July 5, 2025, by and between: Company: Apex Technologies Inc., a corporation with its principal place of business at 123 Innovation Drive, Bengaluru, Karnataka, India (Company), and Employee: Arjun Sharma, an individual residing at 456 Lotus Apartments, Koramangala, Bengaluru, Karnataka, India (Employee). WHEREAS, the Company intends to employ the Employee in the role of Software Engineer, and in the course of employment, the Employee will have access to confidential and proprietary information; WHEREAS, the Company desires to protect such information from unauthorized disclosure; NOW, THEREFORE, in consideration of the Employees employment and the \n",
      "Total characters after final cleaning: 4331\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- FINAL Cleaned Text Preview for doc-4.pdf (First 750 chars) ---\n",
      "NONDISCLOSURE AGREEMENT This NonDisclosure Agreement (Agreement) is made on July 7, 2025, by: Company: Innovate Systems Ltd., a corporation at 101 Cyber City, Gurugram, Haryana, India (Company), and Employee: Rohan Mehra, residing at 654 Orchid Towers, Sector 56, Gurugram, Haryana, India (Employee). WHEREAS, the Employees role as Data Analyst involves access to sensitive information; WHEREAS, the Company requires protection of such information; NOW, THEREFORE, the parties agree: 1. Confidential Information 1.Definition. Confidential Information includes proprietary data such as product designs, financial records, and client contracts disclosed to the Employee. 2. Employee Responsibilities 1.Duties. The Employee shall perform Data Analyst du\n",
      "Total characters after final cleaning: 2500\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- FINAL Cleaned Text Preview for doc-5.pdf (First 750 chars) ---\n",
      "NONDISCLOSURE AGREEMENT This NonDisclosure Agreement (Agreement) is made on July 9, 2025, by: Company: Quantum Dynamics Ltd., a corporation at 555 Tech Hub, Pune, Maharashtra, India (Company), and Employee: Vikram Desai, at 123 Riverfront Villas, Baner, Pune, Maharashtra, India (Employee). WHEREAS, the Employees role as Systems Administrator grants access to confidential data; WHEREAS, the Company requires confidentiality protection; NOW, THEREFORE, the parties agree: 1. Confidential Information 1.Definition. Includes trade secrets, financials, and client information disclosed to the Employee. 2. Employee Obligations 1.Duties. Perform Systems Administrator tasks per Company instructions. 2.NonDisclosure. No disclosure of Confidential Inform\n",
      "Total characters after final cleaning: 2118\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- FINAL Cleaned Text Preview for doc-6.pdf (First 750 chars) ---\n",
      "Page 1 of 4 AGREEMENT OF PURCHASE This agreement is by and between __________________ (Institution ), and _____________________ ( Seller ). WHEREAS, the Seller desires to sell to the Institution a comprehensive collection of ___________________ (Collection ), whi ch is more particularly described in the attached inventory, Attachment A , which is incorporated herein by reference ; and, WHEREAS, the Institution deems it in its interest to acquire the Collection for custodial care and appropriate service to the public , and is agreeable to purchasing the Collection under the terms hereafter stated; NOW, THEREFORE, the parties hereby agree as follows: 1) Purchase . The Seller agrees to sell, and the Institution agrees to buy, the Collection fo\n",
      "Total characters after final cleaning: 7340\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- FINAL Cleaned Text Preview for doc-7.pdf (First 750 chars) ---\n",
      "Ninu, Inc. Public Domain Agreement v.1.01 1 PUBLIC DOMAIN AGREEMENT THIS IS A LEGAL AGREEMENT (\"Agreement\") BETWEEN YOU AND NINU, INC. (NIMIA). PLEASE READ THIS AGREEMENT IN ITS ENTIRETY BEFORE YOU DOWNLOAD OR ACCESS ANY CONTENT CATEGORIZED AS PUBLIC DOMAIN (CONTENT) ON NIMIA OWNED WEB PROPERTIES. BY DOWNLOADING OR RECEIVING ANY CONTENT CATEGORIZED AS PUBLIC DOMAIN FROM NIMIA OWNED WEB PROPERTIES YOU AGREE TO BE BOUND BY THE TERMS OF THIS AGREEMENT. The Public Domain Agreement (Agreement) is a contract between the You and Ninu, Inc. (Nimia), 701 5th Avenue, Suite 4200, Seattle WA, 98104. The Agreement is in addition to the Producer Agreement, Site User Agreement, Cloud Storage Agreement, Application License Agreement, and Community Section \n",
      "Total characters after final cleaning: 16975\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ... (previous code for raw_extracted_texts and basic cleaning loop) ...\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"Starting Advanced Cleaning...\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "final_cleaned_texts = {} # Dictionary to store final cleaned text\n",
    "\n",
    "for file_name, text_content in cleaned_texts.items(): # Use cleaned_texts from previous step\n",
    "    text = text_content # Start with the text that underwent basic cleaning\n",
    "\n",
    "    # --- Advanced Cleaning Steps ---\n",
    "\n",
    "    # Step 4: Handle hyphenated words at line breaks.\n",
    "    # This regex looks for a word character, followed by a hyphen, then optional whitespace,\n",
    "    # and then another word character. It joins them.\n",
    "    # Example: \"agree- ment\" -> \"agreement\"\n",
    "    text = re.sub(r'(\\w+)-\\s*(\\w+)', r'\\1\\2', text)\n",
    "\n",
    "\n",
    "    # Step 5: Remove specific known headers/footers (MOST IMPORTANT: CUSTOMIZE THIS!)\n",
    "    # This is HIGHLY specific to your actual documents. You MUST identify common patterns\n",
    "    # from your document previews and create regex for them.\n",
    "    # Example (hypothetical patterns, you need to find yours):\n",
    "    # text = re.sub(r'Page \\d+ of \\d+', '', text, flags=re.IGNORECASE) # Removes \"Page X of Y\"\n",
    "    # text = re.sub(r'Confidential Agreement \\d{4}', '', text) # Removes \"Confidential Agreement 2023\"\n",
    "    # text = re.sub(r'\\[\\s*LOGO\\s*\\]', '', text, flags=re.IGNORECASE) # Removes placeholder like [ LOGO ]\n",
    "\n",
    "    # Example: If your documents have a consistent footer like \"Document Version 1.0\"\n",
    "    # text = re.sub(r'Document Version \\d+\\.\\d+', '', text)\n",
    "\n",
    "    # Example: If you see specific legal boilerplate that repeats and is not useful for clause extraction\n",
    "    # text = re.sub(r'WHEREAS, the parties agree as follows:', '', text)\n",
    "\n",
    "    # IMPORTANT: Add your specific patterns here based on your actual document content.\n",
    "    # Start with simple patterns and test.\n",
    "\n",
    "\n",
    "    # Step 6: Remove multiple spaces again, just to be sure after other replacements\n",
    "    text = re.sub(r' {2,}', ' ', text)\n",
    "\n",
    "    # Step 7: (Optional) Remove specific leading/trailing symbols that might appear at start/end of document\n",
    "    # For example, if some documents start with an unwanted bullet or symbol\n",
    "    # text = re.sub(r'^\\s*•\\s*', '', text)\n",
    "\n",
    "\n",
    "    # Store the final cleaned text\n",
    "    final_cleaned_texts[file_name] = text\n",
    "\n",
    "    # Print a preview of the final cleaned text\n",
    "    print(f\"--- FINAL Cleaned Text Preview for {file_name} (First 750 chars) ---\")\n",
    "    print(final_cleaned_texts[file_name][:750])\n",
    "    print(f\"Total characters after final cleaning: {len(final_cleaned_texts[file_name])}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc0ada36",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text_folder = os.path.join('..', 'data', 'processed_text')\n",
    "os.makedirs(processed_text_folder, exist_ok=True) # Create folder if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2112e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "Saving Cleaned Documents...\n",
      "==================================================\n",
      "\n",
      "Saved cleaned text for doc-1.pdf to ..\\data\\processed_text\\doc-1.txt\n",
      "Saved cleaned text for doc-2.pdf to ..\\data\\processed_text\\doc-2.txt\n",
      "Saved cleaned text for doc-3.pdf to ..\\data\\processed_text\\doc-3.txt\n",
      "Saved cleaned text for doc-4.pdf to ..\\data\\processed_text\\doc-4.txt\n",
      "Saved cleaned text for doc-5.pdf to ..\\data\\processed_text\\doc-5.txt\n",
      "Saved cleaned text for doc-6.pdf to ..\\data\\processed_text\\doc-6.txt\n",
      "Saved cleaned text for doc-7.pdf to ..\\data\\processed_text\\doc-7.txt\n",
      "\n",
      "All cleaned documents processed and saved!\n"
     ]
    }
   ],
   "source": [
    "# After the main loop that processes all documents and populates final_cleaned_texts\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"Saving Cleaned Documents...\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "for file_name, text_content in final_cleaned_texts.items():\n",
    "    # Change file extension from .pdf to .txt\n",
    "    output_file_name = file_name.replace('.pdf', '.txt')\n",
    "    output_path = os.path.join(processed_text_folder, output_file_name)\n",
    "\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(text_content)\n",
    "        print(f\"Saved cleaned text for {file_name} to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving cleaned text for {file_name}: {e}\")\n",
    "\n",
    "print(\"\\nAll cleaned documents processed and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe04db59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AIzaSyCnXd9e1sG3r_RTXEpLVRfPRl9y-CpxItE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e5ecaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini model initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# --- IMPORTANT: Replace 'YOUR_GEMINI_API_KEY' with your actual API key ---\n",
    "# In a real project, avoid hardcoding keys. Use environment variables or a config file.\n",
    "# But for this quick PoC in Jupyter, this is the fastest way to get started.\n",
    "API_KEY = \"AIzaSyCnXd9e1sG3r_RTXEpLVRfPRl9y-CpxItE\" # <-- PASTE YOUR API KEY HERE\n",
    "\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "# Initialize the model\n",
    "model = genai.GenerativeModel('gemini-1.5-flash') # or 'gemini-1.5-flash'\n",
    "\n",
    "print(\"Gemini model initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83a40aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 processed text files: ['doc-1.txt', 'doc-2.txt', 'doc-3.txt', 'doc-4.txt', 'doc-5.txt', 'doc-6.txt', 'doc-7.txt']\n",
      "Loaded doc-1.txt (Length: 5419 chars)\n",
      "Loaded doc-2.txt (Length: 3074 chars)\n",
      "Loaded doc-3.txt (Length: 4331 chars)\n",
      "Loaded doc-4.txt (Length: 2500 chars)\n",
      "Loaded doc-5.txt (Length: 2118 chars)\n",
      "Loaded doc-6.txt (Length: 7340 chars)\n",
      "Loaded doc-7.txt (Length: 16975 chars)\n",
      "\n",
      "All processed documents loaded into 'loaded_cleaned_texts' dictionary.\n"
     ]
    }
   ],
   "source": [
    "# Define path to your processed text folder\n",
    "processed_text_folder = os.path.join('..', 'data', 'processed_text')\n",
    "\n",
    "# Dictionary to store loaded cleaned texts\n",
    "loaded_cleaned_texts = {}\n",
    "\n",
    "if not os.path.exists(processed_text_folder):\n",
    "    print(f\"Error: Processed text folder not found at {processed_text_folder}.\")\n",
    "else:\n",
    "    txt_files = sorted([f for f in os.listdir(processed_text_folder) if f.lower().endswith('.txt')])\n",
    "\n",
    "    if not txt_files:\n",
    "        print(f\"No .txt files found in {processed_text_folder}. Please ensure you saved them in Phase 2.\")\n",
    "    else:\n",
    "        print(f\"Found {len(txt_files)} processed text files: {txt_files}\")\n",
    "        for txt_file_name in txt_files:\n",
    "            file_path = os.path.join(processed_text_folder, txt_file_name)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    loaded_cleaned_texts[txt_file_name] = f.read()\n",
    "                print(f\"Loaded {txt_file_name} (Length: {len(loaded_cleaned_texts[txt_file_name])} chars)\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {txt_file_name}: {e}\")\n",
    "        print(\"\\nAll processed documents loaded into 'loaded_cleaned_texts' dictionary.\")\n",
    "\n",
    "# You can inspect one document to verify\n",
    "# if loaded_cleaned_texts:\n",
    "#     first_doc_name = list(loaded_cleaned_texts.keys())[0]\n",
    "#     print(f\"\\nPreview of first loaded document ({first_doc_name}):\")\n",
    "#     print(loaded_cleaned_texts[first_doc_name][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6089c63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attempting Clause Extraction for doc-1.txt ---\n",
      "\n",
      "Extracted Parties Clause:\n",
      "Service Provider: [Service Provider Name], a [business entity, e.g., corporation, LLC] with its principal place of business at [Service Provider Address] (Provider), and Client: [Client Name], a [business entity or individual] with its principal place of business or residence at [Client Address] (Client).\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "Extracted Governing Law Clause:\n",
      "This Agreement shall be governed by and construed in accordance with the laws of [State/Country], with venue exclusively in the courts of [City/State].\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "Extracted Structured Data:\n",
      "```json\n",
      "{\n",
      "  \"Parties\": \"Service Provider: [Service Provider Name], a [business entity, e.g., corporation, LLC] with its principal place of business at [Service Provider Address] (Provider), and Client: [Client Name], a [business entity or individual] with its principal place of business or residence at [Client Address] (Client).\",\n",
      "  \"Term\": \"This Agreement shall commence on [Start Date] and continue until the Services are completed, estimated to be on or before [Completion Date], unless terminated earlier in accordance with Section 4.\",\n",
      "  \"Governing Law\": \"This Agreement shall be governed by and construed in accordance with the laws of [State/Country], with venue exclusively in the courts of [City/State].\"\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_doc_name = 'doc-1.txt' # Replace with a document you want to test\n",
    "document_text = loaded_cleaned_texts.get(sample_doc_name, \"Document not found.\")\n",
    "\n",
    "if document_text == \"Document not found.\":\n",
    "    print(f\"Error: {sample_doc_name} not found in loaded_cleaned_texts.\")\n",
    "else:\n",
    "    print(f\"\\n--- Attempting Clause Extraction for {sample_doc_name} ---\")\n",
    "\n",
    "    # Prompt 1: Simple extraction of 'Parties' clause\n",
    "    prompt_parties = f\"\"\"\n",
    "    Extract the 'Parties' clause from the following legal document.\n",
    "    Provide only the text of the clause, without any additional commentary or formatting.\n",
    "\n",
    "    Document:\n",
    "    {document_text}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Assuming 'model' is your initialized Gemini model\n",
    "        response = model.generate_content(prompt_parties)\n",
    "        extracted_parties_clause = response.text\n",
    "        print(\"\\nExtracted Parties Clause:\")\n",
    "        print(extracted_parties_clause)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Gemini API call for Parties clause: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    # Prompt 2: Simple extraction of 'Governing Law' clause\n",
    "    prompt_governing_law = f\"\"\"\n",
    "    Extract the 'Governing Law' clause from the following legal document.\n",
    "    Provide only the text of the clause, without any additional commentary or formatting.\n",
    "\n",
    "    Document:\n",
    "    {document_text}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = model.generate_content(prompt_governing_law)\n",
    "        extracted_governing_law = response.text\n",
    "        print(\"\\nExtracted Governing Law Clause:\")\n",
    "        print(extracted_governing_law)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Gemini API call for Governing Law clause: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    # Prompt 3: Extract multiple clauses in a structured format (more advanced)\n",
    "    prompt_structured_extraction = f\"\"\"\n",
    "    From the following legal document, extract the following clauses:\n",
    "    1. Parties\n",
    "    2. Term\n",
    "    3. Governing Law\n",
    "\n",
    "    Provide the output in a JSON format, where each key is the clause name and the value is the extracted clause text.\n",
    "    If a clause is not found, its value should be \"Not Found\".\n",
    "\n",
    "    Document:\n",
    "    {document_text}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # For structured output, you might need to adjust generationConfig if using specific models\n",
    "        # For gemini-pro, it often tries to follow JSON format if prompted well.\n",
    "        response = model.generate_content(prompt_structured_extraction)\n",
    "        extracted_structured_data = response.text\n",
    "        print(\"\\nExtracted Structured Data:\")\n",
    "        print(extracted_structured_data)\n",
    "        # You might try to parse it to verify\n",
    "        # import json\n",
    "        # parsed_data = json.loads(extracted_structured_data)\n",
    "        # print(parsed_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Gemini API call for structured extraction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b8d6019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attempting Summarization for doc-2.txt ---\n",
      "\n",
      "Concise Summary:\n",
      "This Non-Disclosure Agreement (NDA), effective July 15, 2025, is between Horizon Solutions Pvt. Ltd. and its employee, Neha Patel.  The agreement obligates Patel, as Marketing Manager, to protect Horizon Solutions' confidential information (marketing strategies, client data, etc.) during and for three years after employment.  Patel must not disclose this information without written consent and must return all confidential materials upon termination.  Breach of confidentiality allows Horizon Solutions to immediately terminate the agreement, and Patel agrees to indemnify the company for any resulting losses.  The agreement is governed by Telangana, India law.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "Key Points Summary (Bulleted List):\n",
      "* **Confidentiality of Proprietary Information:**  The employee (Neha Patel) is prohibited from disclosing any confidential information belonging to Horizon Solutions Pvt. Ltd.  This includes marketing strategies, client data, and proprietary processes. This obligation extends for three years after the termination of employment.\n",
      "\n",
      "* **Employee Obligations:** Neha Patel must use confidential information only for her job duties as Marketing Manager and must immediately report any breaches of confidentiality.  She is also responsible for returning all confidential information upon termination of employment.\n",
      "\n",
      "* **Agreement Duration and Termination:** The agreement starts July 15, 2025, and continues during employment plus three years afterward.  The agreement can be terminated immediately by Horizon Solutions if Neha breaches the confidentiality clause.\n",
      "\n",
      "* **Compensation and Benefits:** Neha Patel will receive an annual salary of $50,000, paid monthly, along with performance bonuses and medical coverage as per company policy.\n",
      "\n",
      "* **Liability and Indemnification:** Neha Patel must indemnify Horizon Solutions for any losses resulting from unauthorized disclosure of confidential information. Horizon Solutions' liability is limited to direct damages caused by its breach of the agreement.\n",
      "\n",
      "* **Governing Law:** The agreement is governed by the laws of Telangana, India, and any disputes will be resolved in Hyderabad courts.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_doc_name_summary = 'doc-2.txt' # Choose a document, e.g., an NDA or a longer one\n",
    "document_text_summary = loaded_cleaned_texts.get(sample_doc_name_summary, \"Document not found.\")\n",
    "\n",
    "if document_text_summary == \"Document not found.\":\n",
    "    print(f\"Error: {sample_doc_name_summary} not found in loaded_cleaned_texts.\")\n",
    "else:\n",
    "    print(f\"\\n--- Attempting Summarization for {sample_doc_name_summary} ---\")\n",
    "\n",
    "    prompt_summary_concise = f\"\"\"\n",
    "    Summarize the following legal document in 3-5 concise sentences.\n",
    "    Focus on the main purpose, key parties, and primary obligations.\n",
    "\n",
    "    Document:\n",
    "    {document_text_summary}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = model.generate_content(prompt_summary_concise)\n",
    "        generated_summary_concise = response.text\n",
    "        print(\"\\nConcise Summary:\")\n",
    "        print(generated_summary_concise)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Gemini API call for concise summary: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    # Prompt 2: Key Points Summary (Bullet Points)\n",
    "    prompt_summary_bullets = f\"\"\"\n",
    "    Identify the most important key points from the following legal document.\n",
    "    Present them as a bulleted list.\n",
    "\n",
    "    Document:\n",
    "    {document_text_summary}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = model.generate_content(prompt_summary_bullets)\n",
    "        generated_summary_bullets = response.text\n",
    "        print(\"\\nKey Points Summary (Bulleted List):\")\n",
    "        print(generated_summary_bullets)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Gemini API call for bulleted summary: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "960b0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of few-shot prompt structure\n",
    "prompt_with_examples = f\"\"\"\n",
    "Extract the 'Term' clause from the legal document.\n",
    "Example 1:\n",
    "Document: \"This agreement begins on Jan 1, 2023 and ends on Dec 31, 2024.\"\n",
    "Term: \"This agreement begins on Jan 1, 2023 and ends on Dec 31, 2024.\"\n",
    "\n",
    "Example 2:\n",
    "Document: \"The contract shall commence on the Effective Date and continue for a period of two years.\"\n",
    "Term: \"The contract shall commence on the Effective Date and continue for a period of two years.\"\n",
    "\n",
    "Document:\n",
    "{document_text}\n",
    "Term:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df5737ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document: doc-1.txt\n",
      "Error extracting clauses: Extra data: line 8 column 1 (char 745)\n",
      "Finished processing doc-1.txt.\n",
      "\n",
      "Processing document: doc-2.txt\n",
      "Error extracting clauses: Extra data: line 8 column 1 (char 1088)\n",
      "Finished processing doc-2.txt.\n",
      "\n",
      "Processing document: doc-3.txt\n",
      "Error extracting clauses: Extra data: line 8 column 1 (char 2721)\n",
      "Finished processing doc-3.txt.\n",
      "\n",
      "Processing document: doc-4.txt\n",
      "Error extracting clauses: Extra data: line 8 column 1 (char 913)\n",
      "Finished processing doc-4.txt.\n",
      "\n",
      "Processing document: doc-5.txt\n",
      "Error extracting clauses: Extra data: line 8 column 1 (char 1452)\n",
      "Finished processing doc-5.txt.\n",
      "\n",
      "Processing document: doc-6.txt\n",
      "Error extracting clauses: Extra data: line 8 column 1 (char 392)\n",
      "Finished processing doc-6.txt.\n",
      "\n",
      "Processing document: doc-7.txt\n",
      "Error extracting clauses: Extra data: line 8 column 1 (char 624)\n",
      "Finished processing doc-7.txt.\n"
     ]
    }
   ],
   "source": [
    "def process_legal_document(document_text, model):\n",
    "    \"\"\"\n",
    "    Processes a single legal document to extract clauses and generate a summary.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # --- Clause Extraction ---\n",
    "    # Use your best refined prompts here for each clause\n",
    "    clauses_to_extract = [\"Parties\", \"Term\", \"Governing Law\", \"Confidentiality\"] # Add more as needed\n",
    "\n",
    "    # For structured JSON output for clauses\n",
    "    clause_prompt = f\"\"\"\n",
    "    From the following legal document, extract the following clauses: {', '.join(clauses_to_extract)}.\n",
    "    Provide the output in a JSON format, where each key is the clause name and the value is the extracted clause text.\n",
    "    If a clause is not found, its value should be \"Not Found\".\n",
    "\n",
    "    Document:\n",
    "    {document_text}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = model.generate_content(clause_prompt)\n",
    "        # Assuming the model outputs valid JSON, parse it\n",
    "        import json\n",
    "        extracted_clauses_json = json.loads(response.text.strip('```json').strip('```')) # Remove markdown code block if present\n",
    "        results['extracted_clauses'] = extracted_clauses_json\n",
    "    except Exception as e:\n",
    "        results['extracted_clauses'] = {\"error\": str(e), \"message\": \"Failed to extract clauses.\"}\n",
    "        print(f\"Error extracting clauses: {e}\")\n",
    "\n",
    "    # --- Summarization ---\n",
    "    # Use your best refined prompt for summarization\n",
    "    summary_prompt = f\"\"\"\n",
    "    Summarize the following legal document in 3-5 concise sentences, focusing on the main purpose, key parties, and primary obligations.\n",
    "\n",
    "    Document:\n",
    "    {document_text}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = model.generate_content(summary_prompt)\n",
    "        results['summary'] = response.text\n",
    "    except Exception as e:\n",
    "        results['summary'] = {\"error\": str(e), \"message\": \"Failed to generate summary.\"}\n",
    "        print(f\"Error generating summary: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# --- Loop through all loaded documents ---\n",
    "all_document_results = {}\n",
    "for doc_name, text_content in loaded_cleaned_texts.items():\n",
    "    print(f\"\\nProcessing document: {doc_name}\")\n",
    "    # Add a check for token limit if documents are very long\n",
    "    # Google Gemini has a context window. If document_text is too long, it will fail.\n",
    "    # For 'gemini-1.5-flash', it's 1 million tokens. Your 7 docs are likely fine.\n",
    "\n",
    "    # Call the processing function\n",
    "    doc_results = process_legal_document(text_content, model)\n",
    "    all_document_results[doc_name] = doc_results\n",
    "    print(f\"Finished processing {doc_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77c9d712",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results_folder = os.path.join('..', 'data', 'analysis_results')\n",
    "os.makedirs(analysis_results_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07acc99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "Saving Analysis Results...\n",
      "==================================================\n",
      "\n",
      "Saved analysis for doc-1.txt to ..\\data\\analysis_results\\doc-1_analysis.json\n",
      "Saved analysis for doc-2.txt to ..\\data\\analysis_results\\doc-2_analysis.json\n",
      "Saved analysis for doc-3.txt to ..\\data\\analysis_results\\doc-3_analysis.json\n",
      "Saved analysis for doc-4.txt to ..\\data\\analysis_results\\doc-4_analysis.json\n",
      "Saved analysis for doc-5.txt to ..\\data\\analysis_results\\doc-5_analysis.json\n",
      "Saved analysis for doc-6.txt to ..\\data\\analysis_results\\doc-6_analysis.json\n",
      "Saved analysis for doc-7.txt to ..\\data\\analysis_results\\doc-7_analysis.json\n",
      "\n",
      "All analysis results saved!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"Saving Analysis Results...\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "for doc_name, results in all_document_results.items():\n",
    "    output_json_file = doc_name.replace('.txt', '_analysis.json')\n",
    "    output_path = os.path.join(analysis_results_folder, output_json_file)\n",
    "\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=4) # indent=4 for pretty printing\n",
    "        print(f\"Saved analysis for {doc_name} to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving analysis for {doc_name}: {e}\")\n",
    "\n",
    "print(\"\\nAll analysis results saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56bdd15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
